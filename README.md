# ğŸ§  Gen-Z Mental Wellness â€” ML Pipeline Dashboard

> A dual-target machine learning pipeline comparing **Regression** and **Classification** on Gen-Z mental wellness data, deployed as an interactive Streamlit web app.

---

## ğŸ“Œ Project Overview

This project builds a complete end-to-end ML pipeline on a Gen-Z mental wellness dataset. It simultaneously solves **two different ML problems** using the same set of features:

| Target Variable | Type | Problem |
|----------------|------|---------|
| `Wellbeing_Index` | Continuous score (1â€“10) | Regression |
| `Burnout_Risk` | Low / Medium / High | Classification |

The pipeline follows these steps â€” directly from the whiteboard plan:

```
Data Loading & EDA
      â†“
Feature Engineering (Correlation, PCA, Redundant Feature Detection)
      â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   CLASSIFICATION    â”‚         â”‚     REGRESSION        â”‚
      â”‚   Burnout_Risk      â”‚         â”‚   Wellbeing_Index     â”‚
      â”‚ Step 1: SMOTE       â”‚         â”‚ Step 1: Train/Test    â”‚
      â”‚ Step 2: Pie Charts  â”‚         â”‚        Split          â”‚
      â”‚ Step 3: Scaling     â”‚         â”‚ Step 2: Scaling       â”‚
      â”‚ Step 4: 6 Models +  â”‚         â”‚ Step 3: 6 Models +    â”‚
      â”‚         10-Fold CV  â”‚         â”‚         10-Fold CV    â”‚
      â”‚ Step 5: Metrics     â”‚         â”‚ Step 4: Metrics       â”‚
      â”‚ Step 6: GridSearch  â”‚         â”‚ Step 5: GridSearch    â”‚
      â”‚ Step 7: Feature Imp â”‚         â”‚ Step 6: Feature Imp   â”‚
      â”‚ Step 8: XAI         â”‚         â”‚ Step 7: XAI           â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
              Final Comparison + Live Prediction
```

---

## ğŸ“‚ Project Structure

```
genz-mental-wellness-app/
â”œâ”€â”€ app.py                                      # Streamlit web app
â”œâ”€â”€ requirements.txt                            # Python dependencies
â””â”€â”€ genz_mental_wellness_synthetic_dataset.csv  # Dataset
```

---

## ğŸ“Š Dataset

**File:** `genz_mental_wellness_synthetic_dataset.csv`
**Rows:** 10,000 synthetic samples
**Features:** 20 input features + 2 target variables

### Input Features

| Feature | Type | Description |
|---------|------|-------------|
| `Age` | Numeric | Age of the person (18â€“25) |
| `Gender` | Categorical | Male / Female |
| `Country` | Categorical | Country of residence |
| `Student_Working_Status` | Categorical | Student or Working |
| `Daily_Social_Media_Hours` | Numeric | Hours spent on social media per day |
| `Screen_Time_Hours` | Numeric | Total daily screen time |
| `Night_Scrolling_Frequency` | Numeric | How often scrolling at night (0â€“7) |
| `Online_Gaming_Hours` | Numeric | Daily hours spent gaming |
| `Content_Type_Preference` | Categorical | News / Gaming / Entertainment / Educational |
| `Exercise_Frequency_per_Week` | Numeric | Days per week exercising |
| `Daily_Sleep_Hours` | Numeric | Average sleep hours |
| `Caffeine_Intake_Cups` | Numeric | Daily caffeine cups |
| `Study_Work_Hours_per_Day` | Numeric | Hours studying or working per day |
| `Overthinking_Score` | Numeric | Self-reported overthinking (1â€“10) |
| `Anxiety_Score` | Numeric | Anxiety level (1â€“10) |
| `Mood_Stability_Score` | Numeric | Mood stability (1â€“10) |
| `Social_Comparison_Index` | Numeric | Social comparison tendency (1â€“10) |
| `Sleep_Quality_Score` | Numeric | Sleep quality (1â€“10) |
| `Motivation_Level` | Numeric | Motivation (1â€“10) |
| `Emotional_Fatigue_Score` | Numeric | Emotional fatigue (1â€“10) |

### Target Variables

| Variable | Type | Values |
|----------|------|--------|
| `Wellbeing_Index` | Continuous | 1.0 â€“ 10.0 |
| `Burnout_Risk` | Categorical | Low / Medium / High |

---

## ğŸ”§ Pipeline â€” Step by Step Explanation

### Step 0 â€” Imports & Setup
All required libraries are imported once at the top:
- `scikit-learn` â€” all ML models, metrics, preprocessing
- `imbalanced-learn` â€” SMOTE for class balancing
- `matplotlib` / `seaborn` â€” visualizations
- `pandas` / `numpy` â€” data manipulation

---

### Step 1 â€” Load & Explore Data (EDA)
- Load CSV into a pandas DataFrame
- Check shape, data types, missing values
- View class distribution of `Burnout_Risk`
- View distribution of `Wellbeing_Index`

---

### Step 2 â€” Feature Engineering

#### 2a. Correlation Heatmap
- Encode all categorical columns to numbers
- Compute Pearson correlation between every feature pair
- Plot as a heatmap â€” values close to Â±1 = highly correlated
- Detect redundant features (|r| > 0.85) that carry duplicate information

#### 2b. PCA â€” Principal Component Analysis
- Standardize all numeric features
- Fit PCA to find the directions of maximum variance
- Plot Scree plot (variance per component) and Cumulative variance curve
- Find how many components explain â‰¥95% of variance
- Helps understand dimensionality and feature redundancy

---

### Step 3 â€” Preprocessing

#### One-Hot Encoding
```
Gender: Male/Female  â†’  Gender_Male: 1 or 0
```
Converts categorical columns to binary columns. `drop_first=True` avoids the dummy variable trap.

#### Label Encoding
```
Burnout_Risk: High/Low/Medium  â†’  0 / 1 / 2
```
Converts classification target to integers for sklearn.

#### Train/Test Split
- 80% training, 20% testing
- `stratify=y` for classification â€” preserves class ratio in both splits
- `random_state=42` â€” reproducible results every run

---

## ğŸ”´ Part A â€” Classification (Burnout_Risk)

### Step 1 â€” SMOTE (Synthetic Minority Over-sampling Technique)
The dataset has class imbalance â€” far more "Medium" samples than "Low" or "High". SMOTE fixes this:

```
Before SMOTE:  Low=64   Medium=7200  High=736
After SMOTE:   Low=7200 Medium=7200  High=7200
```

**How SMOTE works:**
1. Picks a minority class sample
2. Finds its K nearest neighbors (also minority class)
3. Creates a new synthetic point **between** them
4. Repeats until all classes are equal size

âš ï¸ SMOTE is applied **only to training data** â€” never to test data.

---

### Step 2 â€” Pie Charts (Before & After SMOTE)
Visual confirmation of class balancing â€” shows the distribution before and after SMOTE on the training set.

---

### Step 3 â€” StandardScaler
```
z = (x - mean) / std
```
Every feature is rescaled to mean=0, std=1.

- `fit_transform` on training data â€” **learns** mean and std from training
- `transform` only on test data â€” **applies** the same values (prevents data leakage)

**Why needed:** SVM and KNN are distance-based â€” a feature with range 0â€“10000 would dominate one with range 0â€“1.

---

### Step 4 â€” Six Classifiers with 10-Fold Cross-Validation

| Model | How It Works |
|-------|-------------|
| **Logistic Regression** | Sigmoid function on linear combination of features. Fast and interpretable |
| **Decision Tree** | Splits data by yes/no questions on feature values |
| **Random Forest** | 100s of trees on random data subsets â€” majority vote wins |
| **Gradient Boosting** | Sequential trees, each fixing errors of the previous |
| **SVM** | Finds maximum-margin hyperplane separating classes |
| **KNN** | Classifies by majority vote of K nearest neighbors |

**10-Fold Cross-Validation:**
```
Training data split into 10 equal folds:
[F1][F2][F3][F4][F5][F6][F7][F8][F9][F10]

Round 1:  Train on F2â€“F10, Test on F1  â†’ Score
Round 2:  Train on F1,F3â€“F10, Test on F2 â†’ Score
...
Round 10: Train on F1â€“F9, Test on F10 â†’ Score

Final result: mean Â± std of all 10 scores
```
Uses `StratifiedKFold` to maintain class ratio in each fold.

---

### Step 5 â€” Evaluation Metrics

| Metric | Formula | Meaning |
|--------|---------|---------|
| **Accuracy** | Correct / Total | % of all predictions that were right |
| **Precision** | TP / (TP + FP) | Of all predicted Highs, how many were actually High? |
| **Recall** | TP / (TP + FN) | Of all actual Highs, how many did we catch? |
| **F1-Score** | 2Ã—(PÃ—R)/(P+R) | Balance between precision and recall |
| **ROC-AUC** | Area under ROC curve | How well model separates classes at all thresholds |

`average='weighted'` â€” computes each metric per class then weights by class size.

**Confusion Matrix:** 3Ã—3 grid showing predicted vs actual for all classes.

---

### Step 6 â€” GridSearchCV (Hyper-Parameter Tuning)
Tries every combination of hyperparameters:
```
Random Forest example:
n_estimators: [100, 200]      â†’ 2 options
max_depth:    [None, 10, 20]  â†’ 3 options
min_samples_split: [2, 5]     â†’ 2 options

Total: 2 Ã— 3 Ã— 2 = 12 combinations
Each with 10-fold CV = 120 model fits
```
Picks the combination with the highest weighted F1-score.

---

### Step 7 â€” Feature Importance (Random Forest)
Random Forest tracks how much each feature reduces **Gini impurity** across all trees.
- Higher value = feature is more useful for prediction
- Scores sum to 1.0 across all features
- Top 15 features plotted as a horizontal bar chart

---

### Step 8 â€” Explainable AI (Permutation Importance)
Model-agnostic approach that works for **any model**:
1. Measure baseline F1 on test data
2. Randomly shuffle one feature (destroys its signal)
3. Measure how much F1 drops
4. Large drop = important feature
5. Repeat 20 times per feature and average for stability

```
Feature shuffled â†’ F1 drops from 0.91 to 0.74 â†’ importance = 0.17
Feature shuffled â†’ F1 drops from 0.91 to 0.90 â†’ importance = 0.01 (not important)
```

---

## ğŸ”µ Part B â€” Regression (Wellbeing_Index)

### Key Difference
- No SMOTE â€” target is continuous, not categorical
- No `stratify` in train/test split
- Uses `KFold` instead of `StratifiedKFold`

### Six Regressors

| Model | How It Works |
|-------|-------------|
| **Linear Regression** | Fits line minimizing sum of squared errors |
| **Ridge** | Linear + L2 penalty, shrinks large coefficients |
| **Lasso** | Linear + L1 penalty, can zero out coefficients (feature selection) |
| **Decision Tree** | Predicts mean value of samples in each leaf node |
| **Random Forest** | Averages predictions from many decision trees |
| **Gradient Boosting** | Sequential trees fitting residual errors |

### Regression Metrics

| Metric | Formula | Meaning |
|--------|---------|---------|
| **MAE** | mean(\|actual - predicted\|) | Average absolute error â€” easy to interpret |
| **RMSE** | sqrt(mean((actual - predicted)Â²)) | Penalizes large errors more than MAE |
| **RÂ²** | 1 - SS_res/SS_tot | % of variance explained (1.0 = perfect, 0 = no better than mean) |

### Actual vs Predicted Plot
Scatter plot where each dot = one test sample.
- X-axis = actual Wellbeing_Index value
- Y-axis = predicted value
- Red dashed diagonal = perfect prediction line
- Points close to diagonal = good model

---

## ğŸ§  Explainable AI Tab

Shows 4 charts side by side:

| Chart | Model | Metric |
|-------|-------|--------|
| Feature Importance â€” Classification | Random Forest | Gini impurity reduction |
| Feature Importance â€” Regression | Random Forest | Gini impurity reduction |
| Permutation Importance â€” Classification | Best classifier | F1 drop |
| Permutation Importance â€” Regression | Best regressor | RÂ² drop |

---

## ğŸ”® Predict Tab

Input sliders for all 20 features â†’ click **Predict** â†’ get:
- **Burnout Risk** classification (Low / Medium / High) with color indicator
- **Wellbeing Index** predicted score
- **Probability bar chart** showing confidence for each class

---

## ğŸ–¥ï¸ App Structure (Tabs)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§  Gen-Z Mental Wellness â€” ML Pipeline Dashboard       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SIDEBAR     â”‚                                          â”‚
â”‚              â”‚  ğŸ“Š EDA & Feature Engineering            â”‚
â”‚ ğŸ“‚ Upload    â”‚     Dataset overview, correlation        â”‚
â”‚    CSV       â”‚     heatmap, PCA analysis                â”‚
â”‚              â”‚                                          â”‚
â”‚ Test Size    â”‚  ğŸ”´ Classification                       â”‚
â”‚ CV Folds     â”‚     SMOTE, 6 models, metrics,            â”‚
â”‚              â”‚     confusion matrix, ROC-AUC            â”‚
â”‚ âœ… Model 1   â”‚                                          â”‚
â”‚ âœ… Model 2   â”‚  ğŸ”µ Regression                           â”‚
â”‚ âœ… Model 3   â”‚     6 regressors, MAE/RMSE/RÂ²,           â”‚
â”‚ ...          â”‚     actual vs predicted                  â”‚
â”‚              â”‚                                          â”‚
â”‚ â–¶ Run        â”‚  ğŸ§  Explainable AI                       â”‚
â”‚  Pipeline    â”‚     Feature importance,                  â”‚
â”‚              â”‚     permutation importance               â”‚
â”‚              â”‚                                          â”‚
â”‚              â”‚  ğŸ”® Predict                              â”‚
â”‚              â”‚     Input sliders â†’ live prediction      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Installation & Running Locally

```bash
# 1. Clone the repo
git clone https://github.com/YOUR_USERNAME/genz-mental-wellness-app.git
cd genz-mental-wellness-app

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the app
streamlit run app.py
```

App opens at: `http://localhost:8501`

---

## â˜ï¸ Deployment (Streamlit Cloud)

1. Push all 3 files to a **public** GitHub repository
2. Go to [share.streamlit.io](https://share.streamlit.io)
3. Sign in with GitHub
4. Click **New app** â†’ select your repo â†’ set main file as `app.py`
5. Click **Deploy**

Live URL format:
```
https://YOUR_USERNAME-genz-mental-wellness-app.streamlit.app
```

---

## ğŸ“¦ Dependencies

```txt
streamlit        â€” web app framework
numpy            â€” numerical computations
pandas           â€” data manipulation
matplotlib       â€” base plotting
seaborn          â€” statistical visualizations
scikit-learn     â€” ML models, metrics, preprocessing
imbalanced-learn â€” SMOTE for class balancing
```

---

## ğŸ”‘ Key Concepts Summary

| Concept | Purpose |
|---------|---------|
| SMOTE | Synthetic oversampling to fix class imbalance |
| StandardScaler | Normalize features to mean=0, std=1 |
| Train/Test Split | Simulate real-world unseen data evaluation |
| 10-Fold CV | Reliable performance estimate using all training data |
| GridSearchCV | Find the best hyperparameters automatically |
| F1-Score | Best metric for imbalanced classification |
| RÂ² Score | Best single metric for regression quality |
| Feature Importance | Which features the model uses most (tree-based) |
| Permutation Importance | Model-agnostic feature contribution on test data |
| PCA | Reduce dimensions while preserving maximum variance |
| Correlation Heatmap | Identify redundant / multicollinear features |
| ROC-AUC | How well model separates classes across all thresholds |
| Confusion Matrix | Detailed breakdown of correct and incorrect predictions |

---